{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341ffd16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T10:06:21.210051Z",
     "start_time": "2023-11-14T10:06:20.123829Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from robotoddler.utils import *\n",
    "# from models.cnn_ac import BinaryCnnAC\n",
    "# from models.cnn_ac_value import BinaryCnnValue\n",
    "# from core.a2c import a2c_step\n",
    "# from core.common import estimate_advantages\n",
    "# from core.agent import Agent\n",
    "import assembly_gymenv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90fead5",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:06:21.854701Z",
     "start_time": "2023-11-14T10:06:21.210585Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Sep 11 2023 10:08:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blocks': tensor([], size=(0, 4)), 'obstacles': tensor([[0.5250, 0.5000, 0.0217],\n",
      "        [0.5250, 0.5000, 0.0650]]), 'targets': tensor([[0.5250, 0.5000, 0.1083]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001B[0m\n",
      "  logger.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001B[0m\n",
      "  logger.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001B[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('assembly_gymenv/AssemblyGymEnv-v0')\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349bba13",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:06:47.541331Z",
     "start_time": "2023-11-14T10:06:47.096539Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m action \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m7\u001B[39m]\n\u001B[0;32m----> 2\u001B[0m next_state, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(next_state)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/wrappers/env_checker.py:37\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menv_step_passive_checker\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:214\u001B[0m, in \u001B[0;36menv_step_passive_checker\u001B[0;34m(env, action)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001B[39;00m\n\u001B[0;32m--> 214\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    216\u001B[0m     result, \u001B[38;5;28mtuple\u001B[39m\n\u001B[1;32m    217\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpects step result to be a tuple, actual type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(result) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "File \u001B[0;32m~/SDSC/Projects/Robotoddler/robotoddler-dev/bullet_training/assembly_gymenv/envs/gym_env.py:147\u001B[0m, in \u001B[0;36mAssemblyGymEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    145\u001B[0m pos \u001B[38;5;241m=\u001B[39m action[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    146\u001B[0m orien \u001B[38;5;241m=\u001B[39m action[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 147\u001B[0m block_type \u001B[38;5;241m=\u001B[39m \u001B[43maction\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    148\u001B[0m info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massembly_env\u001B[38;5;241m.\u001B[39minteract(block_type, pos, orien) \u001B[38;5;66;03m# action = (pos, orien)\u001B[39;00m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;66;03m#if type(action) is list: # discrete relative action\u001B[39;00m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;66;03m#    info_output, pos, phi = self.assembly_env.interact_relative(action[0], action[1], action[2])\u001B[39;00m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;66;03m#else:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;66;03m#delta_index = action[1]\u001B[39;00m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;66;03m#output, updated_pos = self.assembly_env.interact_relative(block_index, delta_index)\u001B[39;00m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "action = [0,7]\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.assembly_env.get_image()\n",
    "plt.imshow(np.flip(img.transpose(), 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94521eed",
   "metadata": {},
   "source": [
    "# Training with Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9cb117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T10:07:08.822404Z",
     "start_time": "2023-11-14T10:07:08.593396Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch_geometric.nn import GCNConv, GraphConv\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "env = gym.make(\"assembly_gymenv/AssemblyGymEnv-v0\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4d548a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T10:07:10.255225Z",
     "start_time": "2023-11-14T10:07:10.251536Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphCore(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers):\n",
    "        super(GraphCore, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphConv(emb_dim, emb_dim, aggr='mean') for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class EPD_GNN(nn.Module):\n",
    "    def __init__(self, num_node_features, num_actions, emb_dim, num_layers):\n",
    "        super(EPD_GNN, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.encode_layer = nn.Sequential(\n",
    "            nn.Linear(num_node_features, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "        self.core = GraphCore(emb_dim, num_layers)\n",
    "        self.decode_layer = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): #edge_index\n",
    "        # Encode\n",
    "        x = self.encode_layer(x)\n",
    "\n",
    "        # Recurrent Full Graph Core\n",
    "        num_objects = x.shape[0]\n",
    "        edge_index = torch.tensor([np.hstack([[i]*num_objects for i in range(num_objects)]), np.hstack([list(np.arange(num_objects))*num_objects])])\n",
    "        x = self.core(x, edge_index)\n",
    "\n",
    "        # Decode (classification)\n",
    "        x = self.decode_layer(x[:-1]) # remove target\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7e6208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T10:07:11.183823Z",
     "start_time": "2023-11-14T10:07:11.180103Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    \n",
    "class PrioritizedReplayBuffer(object):\n",
    "    def __init__(self, max_size, alpha=0.6):\n",
    "        self.max_size = max_size\n",
    "        self.alpha = alpha\n",
    "        self.memory = []\n",
    "        self.priorities = np.zeros(max_size, dtype=np.float32)\n",
    "        self.idx = 0\n",
    "\n",
    "    def push(self, state, action, next_state, reward, priority):\n",
    "        if len(self.memory) < self.max_size:\n",
    "            self.memory.append(Transition(state, action, next_state, reward))\n",
    "        else:\n",
    "            self.memory[self.idx] = Transition(state, action, next_state, reward)\n",
    "        self.priorities[self.idx] = priority\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        priorities = self.priorities[:len(self.memory)]\n",
    "        priorities = priorities ** self.alpha\n",
    "        prob = priorities / priorities.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=prob)\n",
    "        weights = (len(self.memory) * prob[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        experiences = [self.memory[idx] for idx in indices]\n",
    "        return indices, experiences, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for i, priority in zip(indices, priorities):\n",
    "            self.priorities[i] = priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc84f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T10:07:12.604570Z",
     "start_time": "2023-11-14T10:07:12.527937Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AssemblyGymEnv' object has no attribute 'num_actions'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m LR \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-4\u001B[39m \u001B[38;5;66;03m# 1e-4\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Get number of actions per object\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m n_actions \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_actions\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Get the number of state observations\u001B[39;00m\n\u001B[1;32m     23\u001B[0m state \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/core.py:241\u001B[0m, in \u001B[0;36mWrapper.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccessing private attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is prohibited\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/core.py:241\u001B[0m, in \u001B[0;36mWrapper.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccessing private attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is prohibited\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/robotoddler/lib/python3.10/site-packages/gym/core.py:241\u001B[0m, in \u001B[0;36mWrapper.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccessing private attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is prohibited\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'AssemblyGymEnv' object has no attribute 'num_actions'"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "#BATCH_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "num_updates = 25\n",
    "\n",
    "GAMMA = 0.99 # 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "#TAU = 0.005\n",
    "target_update_freq = 200\n",
    "LR = 1e-4 # 1e-4\n",
    "\n",
    "# Get number of actions per object\n",
    "n_actions = env.num_actions\n",
    "# Get the number of state observations\n",
    "state = env.reset()\n",
    "n_observations = len(state[0]) # num of features per object (should be 6)\n",
    "\n",
    "policy_net = EPD_GNN(n_observations, n_actions, 64, 2).to(device)\n",
    "target_net = EPD_GNN(n_observations, n_actions, 64, 2).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=False)\n",
    "\n",
    "prioritized_memory = False\n",
    "if prioritized_memory:\n",
    "    alpha = 0.\n",
    "    beta = 0.\n",
    "    memory = PrioritizedReplayBuffer(30000, alpha) #ReplayMemory(30000)\n",
    "else:\n",
    "    memory = ReplayMemory(30000)\n",
    "\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state, with_randomness=True):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    if with_randomness:\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    else:\n",
    "        eps_threshold = 0\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            #return policy_net(state).max(1)[1].view(1, 1)\n",
    "            q_values = policy_net(state)\n",
    "            target_block = q_values.max(dim=1).values.argmax()\n",
    "            delta_index = q_values[target_block].argmax()\n",
    "            #action = state[target_block,0] + (-0.49 + 0.98 * delta_index / (q_values.shape[1] - 1)) * state[target_block,2]\n",
    "            return torch.tensor([target_block, delta_index], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        print(\"Random action\")\n",
    "        rnd_obj = np.random.randint(state.shape[0]-1)\n",
    "        rnd_delta = np.random.randint(policy_net.num_actions)\n",
    "        return torch.tensor([rnd_obj, rnd_delta], device=device, dtype=torch.long)\n",
    "        #rnd_action = torch.rand(1)\n",
    "        #return torch.tensor(rnd_action, device=device)\n",
    "        #return torch.tensor([[rnd_action]], device=device)#, dtype=torch.long)\n",
    "\n",
    "        \n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(batch_size):\n",
    "    # Implementation with batchsize=1 !\n",
    "    if len(memory.memory) < batch_size:\n",
    "        return None, None\n",
    "    if prioritized_memory:\n",
    "        batch_indices, transitions, batch_weights = memory.sample(batch_size, beta)\n",
    "    else:\n",
    "        transitions = memory.sample(batch_size)\n",
    "        \n",
    "    \n",
    "    loss = 0\n",
    "    for i, t in enumerate(transitions):\n",
    "        s = t.state\n",
    "        a = t.action\n",
    "        r = t.reward\n",
    "        next_s = t.next_state\n",
    "        \n",
    "        state_action_value = policy_net(s)[a[0], a[1]]\n",
    "        \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_value = 0 #torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            if next_s is not None:\n",
    "                next_state_value = target_net(next_s).max() #target_net(next_s).max(1)[0][0]\n",
    "        \n",
    "        expected_state_action_value = (next_state_value * GAMMA) + r\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        \n",
    "        td_error = criterion(state_action_value, expected_state_action_value)\n",
    "        if prioritized_memory:\n",
    "            loss += batch_weights[i] * td_error\n",
    "            memory.update_priorities([batch_indices[i]], [td_error.item()])\n",
    "        else:\n",
    "            loss += td_error\n",
    "    \n",
    "    loss /= batch_size\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss, transitions\n",
    "\n",
    "    \n",
    "def run_episode(policy_net, target=None):\n",
    "    state = env.reset()\n",
    "    if target is not None:\n",
    "        env.target = [target]\n",
    "        state = env.get_observation()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    print(\"Target at {}\".format(state[1,0]))\n",
    "    tot_reward = 0\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = select_action(state, with_randomness=False)\n",
    "        target_block = action[0].item()\n",
    "        delta_index = action[1].item()\n",
    "        observation, reward, terminated, _ = env.step([target_block, delta_index])\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "        tot_reward += reward\n",
    "    \n",
    "    success = 0\n",
    "    if reward > 0: # means the episode was a success\n",
    "        success = 1\n",
    "    \n",
    "    return tot_reward, success\n",
    "        \n",
    "        \n",
    "def test(policy_net, num_episodes=5):\n",
    "    print(\"TEST\")\n",
    "    avg_reward = 0\n",
    "    success_rate = 0\n",
    "    for n in range(num_episodes):\n",
    "        tot_reward, success = run_episode(policy_net)\n",
    "        avg_reward += tot_reward\n",
    "        success_rate += success\n",
    "    return avg_reward / num_episodes, success_rate / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 601\n",
    "else:\n",
    "    num_episodes = 4001\n",
    "\n",
    "test_freq = 500\n",
    "\n",
    "if 0:\n",
    "    test_rewards = []\n",
    "    test_success = []\n",
    "    tot_reward = 0\n",
    "    episode_rewards = []\n",
    "    Q_losses = []\n",
    "\n",
    "    best_policy_param = policy_net.state_dict()\n",
    "    best_test_acc = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    print(\"Episode {}\".format(i_episode))\n",
    "        \n",
    "    if i_episode > 5:\n",
    "        for j in range(num_updates):\n",
    "            loss, transitions = optimize_model(BATCH_SIZE)\n",
    "            if loss is not None:\n",
    "                Q_losses.append(loss.item())\n",
    "        \n",
    "    if i_episode % target_update_freq == 0 and i_episode > 1:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    if i_episode % test_freq == 0 and i_episode > 1:\n",
    "        r, s = test(policy_net, num_episodes=20)\n",
    "        test_rewards.append(r)\n",
    "        test_success.append(s)\n",
    "        if s > best_test_acc:\n",
    "            best_test_acc = s\n",
    "            best_policy_param = policy_net.state_dict()\n",
    "        \n",
    "    # Initialize the environment and get it's state\n",
    "    state = env.reset()\n",
    "    print(\"Target location: {}\".format(env.target[0]))\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        target_block = action[0].item()\n",
    "        delta_index = action[1].item()\n",
    "        observation, reward, terminated, _ = env.step([target_block, delta_index])\n",
    "        tot_reward += reward\n",
    "        reward = torch.tensor(reward, device=device)\n",
    "        done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "            episode_rewards.append(tot_reward)\n",
    "            tot_reward = 0\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        if prioritized_memory:\n",
    "            memory.push(state, action, next_state, reward, 10) # push first with high priority\n",
    "        else:\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "        \n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46484b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_success)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(episode_rewards)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(200), 'valid') / 200, 'r')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Q_losses, '+')\n",
    "plt.plot(np.convolve(Q_losses, np.ones(200), 'valid') / 200, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc35818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfde231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
