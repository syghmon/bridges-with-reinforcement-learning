{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:24:42.941671Z",
     "start_time": "2024-02-19T13:24:41.404776Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from aim import Figure, Image, Run\n",
    "\n",
    "from assembly_gym.envs.assembly_env import AssemblyEnv, Shape, Block\n",
    "from assembly_gym.envs.gym_env import AssemblyGym, sparse_reward, tower_setup, bridge_setup, hard_tower_setup\n",
    "from assembly_gym.utils.geometry import align_frames_2d\n",
    "from assembly_gym.utils.rendering import plot_assembly_env, render_assembly_env\n",
    "\n",
    "from robotoddler.utils.actions import generate_actions\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502333d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image-based features\n",
    "\n",
    "def get_state_features(observation, xlim=(0, 1), ylim=(0, 1), width=512, height=512):\n",
    "    return render_blocks(observation['blocks'], xlim=xlim, ylim=ylim, width=width, height=height)\n",
    "\n",
    "def get_obstacle_features(env, xlim=(0, 1), ylim=(0, 1), width=512, height=512):\n",
    "    return render_blocks(env.assembly_env.obstacles, xlim=xlim, ylim=ylim, width=width, height=height)\n",
    "\n",
    "def action_image_features(env, actions, xlim=(0,1), ylim=(0, 1), width=512, height=512):\n",
    "    blocks = [env.create_block(action) for action in actions]\n",
    "    return [render_blocks([block], xlim=xlim, ylim=ylim, width=width, height=height) for block in blocks]\n",
    "\n",
    "def get_binary_features(observation):\n",
    "    return np.array([\n",
    "        observation['stable'],\n",
    "        observation['collision'], # ToDo obstacle vs block collision\n",
    "        observation['collision_block'],\n",
    "        observation['collision_obstacle'],\n",
    "        observation['collision_floor'],\n",
    "        observation['collision_boundary'],\n",
    "    ])\n",
    "\n",
    "def get_task_features(env, xlim=(0, 1), ylim=(0, 1), width=512, height=512):\n",
    "    cube = Shape(urdf_file='../assembly_gym/shapes/cube.urdf')\n",
    "    target_blocks = [Block(shape=cube, position=target) for target in env.targets]\n",
    "    return render_blocks(target_blocks, xlim=xlim, ylim=ylim, width=width, height=height)\n",
    "\n",
    "def render_blocks(blocks, xlim, ylim, width=512, height=512):\n",
    "    image = np.zeros((width, height), dtype=bool)\n",
    "    X, Y = np.meshgrid(np.linspace(*xlim, image.shape[0]), np.linspace(*ylim, image.shape[1]))\n",
    "    positions = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    for block in blocks:\n",
    "        image = image | block.contains_2d(positions).reshape(image.shape)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_available_actions(state_features, obstacle_features, available_actions, action_features):\n",
    "    \"\"\" \n",
    "    helper function to prune available actions based on immediate collisions checking image overlap\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(len(available_actions), dtype=torch.bool)\n",
    "    reduced_available_actions= []\n",
    "    for i, action in enumerate(available_actions):\n",
    "        if torch.sum(action_features[i] * state_features) == 0 and torch.sum(action_features[i] * obstacle_features) == 0:\n",
    "            mask[i] = True\n",
    "            reduced_available_actions.append(action)\n",
    "    \n",
    "    return reduced_available_actions, action_features[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312740ca26943a51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:24:42.957916Z",
     "start_time": "2024-02-19T13:24:42.942552Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # load shapes\n",
    "# trapezoid = Shape(urdf_file='../assembly_gym/shapes/trapezoid.urdf')\n",
    "# vblock = Shape(urdf_file='../assembly_gym/shapes/v_block.urdf')\n",
    "# cube = Shape(urdf_file='../assembly_gym/shapes/cube.urdf')\n",
    "# tblock = Shape(urdf_file='../assembly_gym/shapes/t_block.urdf')\n",
    "\n",
    "# trapezoid.num_faces_2d, trapezoid.urdf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251dd0099916c909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:24:43.267946Z",
     "start_time": "2024-02-19T13:24:42.956189Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# setup environment and test features\n",
    "width = height = 64\n",
    "\n",
    "env = AssemblyGym(**tower_setup(), \n",
    "                  reward_fct=sparse_reward,\n",
    "                  restrict_2d=True, \n",
    "                  assembly_env=AssemblyEnv(render=False))\n",
    "plot_assembly_env(env)\n",
    "\n",
    "x_discr_ground=np.linspace(0, 1, 10)\n",
    "x_block_offset=[0.]\n",
    "\n",
    "obs, info = env.reset()\n",
    "available_actions = [*generate_actions(env, x_discr_ground, x_block_offset)]\n",
    "action = available_actions[5]\n",
    "\n",
    "# make the state a bit more interesting\n",
    "obs, _, _, _, info = env.step(action)\n",
    "available_actions = [*generate_actions(env, x_discr_ground, x_block_offset)]\n",
    "\n",
    "# get all the features\n",
    "state_features = get_state_features(obs, width=width, height=height)\n",
    "task_features = get_task_features(env, width=width, height=height)\n",
    "obstacle_features = get_obstacle_features(env, width=width, height=height)\n",
    "action_features = action_image_features(env, available_actions, width=width, height=height)\n",
    "binary_features = get_binary_features(obs)\n",
    "print(binary_features)\n",
    "\n",
    "# plotting / illustration\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "ax1.imshow(task_features, cmap='gray')\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_title('Task Features')\n",
    "\n",
    "ax2.imshow(state_features, cmap='gray')\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_title('State Features')\n",
    "\n",
    "ax3.imshow(obstacle_features, cmap='gray')\n",
    "ax3.invert_yaxis()\n",
    "ax3.set_title('Obstacle Features')\n",
    "\n",
    "ax4.imshow(action_features[4], cmap='gray')\n",
    "ax4.invert_yaxis()\n",
    "ax4.set_title('Action Features')\n",
    "\n",
    "env.assembly_env.disconnect_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051ab2f9bd90d97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:24:43.271859Z",
     "start_time": "2024-02-19T13:24:43.264556Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# replay buffer definition\n",
    "# because the available actions depend on the state, we need to store a bit more to avoid re-evaluting the environment when training\n",
    "\n",
    "# noinspection PyTypeChecker\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state_features',\n",
    "                         'binary_features',\n",
    "                         'action',\n",
    "                         'action_features', \n",
    "                         'reward',\n",
    "                         'next_state_features', \n",
    "                         'next_binary_features',\n",
    "                         'next_available_actions',\n",
    "                         'next_actions_features',\n",
    "                         'task_features',\n",
    "                         'obstacle_features',\n",
    "                         'done'))\n",
    "\n",
    "def tensor_size_MB(a):\n",
    "    return a.element_size() * a.nelement() / 1024 / 1024\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.episode = 0\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def size_in_MB(self):\n",
    "        if len(self.memory) == 0:\n",
    "            return 0\n",
    "        size_of_first_element = 0\n",
    "        \n",
    "        for field in Transition._fields:\n",
    "            if isinstance(getattr(self.memory[0], field), torch.Tensor):\n",
    "                size_of_first_element += tensor_size_MB(getattr(self.memory[0], field))\n",
    "        return size_of_first_element * len(self.memory)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e3d5a3152f71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:34:46.462876Z",
     "start_time": "2024-02-19T13:34:46.440292Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we define two models\n",
    "# - one conv net that maps input states to q-values and successor features for only the binary variables (not the images)\n",
    "# - one UNet that maps input states to q-values and successor features for the images\n",
    "# (Johannes) The Unet does currently not work well, it's probably not a good architecture for greating images \n",
    "# that significantly deviate from the input, or something else is going wrong ...\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        # x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_d, out_d, hidden_d=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_d, hidden_d)\n",
    "        self.fc2 = nn.Linear(hidden_d, out_d)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, add_skip=True):\n",
    "        super().__init__()\n",
    "        self.add_skip = add_skip\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = ConvBlock((1 + int(bool(add_skip))) * out_c, out_c)\n",
    "        \n",
    "        # fully connected upsampling of the features\n",
    "        # self.feature_up = mlp()\n",
    "\n",
    "    def forward(self, inputs, skip=None):\n",
    "        x = self.up(inputs)\n",
    "        if self.add_skip:\n",
    "            x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SuccessorNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A MLP with bottle net that predicts successor images and features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, img_size=(512, 512), num_features=6, hidden_dims=None):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [128, 64, 128]\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        cur_dim = in_channels * img_size[0] * img_size[1] + num_features\n",
    "        last_dim = 2 * img_size[0] * img_size[1] + 2 * num_features\n",
    "\n",
    "        for hidden_dim in hidden_dims + [last_dim]:\n",
    "            self.layers.append(nn.Linear(cur_dim, hidden_dim))\n",
    "            cur_dim = hidden_dim\n",
    "\n",
    "    def forward(self, state, features, action, task, obstacles):\n",
    "        img_size = self.img_size\n",
    "        x = torch.cat([state, action, task, obstacles], dim=1).view(state.shape[0], -1)\n",
    "        x = torch.cat([x, features], dim=1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = self.relu(layer(x))\n",
    "        \n",
    "        img_out = x[:, :2 * img_size[0] * img_size[1]].view(-1, 2, img_size[0], img_size[1])\n",
    "        features_out = x[:, 2 * img_size[0] * img_size[1]:].view(-1, 2, features.shape[1])\n",
    "        return img_out, features_out\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, img_size=(512, 512), num_features=6, add_skip=True):\n",
    "        super().__init__()\n",
    "        self.add_skip = add_skip\n",
    "\n",
    "        \"\"\" 1st Encoder \"\"\"\n",
    "        self.e0 = EncoderBlock(in_channels, 16)\n",
    "        self.e1 = EncoderBlock(16, 32)\n",
    "        self.e2 = EncoderBlock(32, 64)\n",
    "        self.e3 = EncoderBlock(64, 128)\n",
    "        # self.e4 = EncoderBlock(128, 256)\n",
    "        \n",
    "        \n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        # self.b = ConvBlock(256, 512)\n",
    "        self.b = ConvBlock(128, 256)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        # self.d1 = DecoderBlock(512, 256, add_skip=add_skip)\n",
    "        self.d2 = DecoderBlock(256, 128, add_skip=add_skip)\n",
    "        self.d3 = DecoderBlock(128, 64, add_skip=add_skip)\n",
    "        self.d4 = DecoderBlock(64, 32, add_skip=add_skip)\n",
    "        self.d5 = DecoderBlock(32, 16, add_skip=add_skip)\n",
    "        \n",
    "        self.out = nn.Conv2d(16, 2, kernel_size=1, padding=0)\n",
    "\n",
    "        # compute bottleneck size\n",
    "        x = torch.zeros(1, in_channels, *img_size)\n",
    "        with torch.no_grad():\n",
    "            x = self.e0(x)[-1]\n",
    "            x = self.e1(x)[-1]\n",
    "            x = self.e2(x)[-1]\n",
    "            x = self.e3(x)[-1]\n",
    "            # x = self.e4(x)[-1]\n",
    "            x = self.b(x)\n",
    "            self.bottleneck_size = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "\n",
    "        self.mlp = MLP(self.bottleneck_size + num_features, 2 * num_features)\n",
    "        \n",
    "    def forward(self, state, features, action, task, obstacles):\n",
    "        # print(state.shape, obstacles.shape, action.shape, task.shape)\n",
    "        input = torch.cat([state, action, task, obstacles], dim=1)\n",
    "        \n",
    "        # print(\"min/max\", input.min().item(), input.max().item())\n",
    "        # print(input.shape)\n",
    "        s0, p0 = self.e0(input)\n",
    "        # print(\"min/max\", p0.min().item(), p0.max().item())\n",
    "        \"\"\" 1st Encoder \"\"\"\n",
    "        s1, p1 = self.e1(p0)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        # s4, p4 = self.e4(p3)\n",
    "        \n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        # b = self.b(p4)\n",
    "        b = self.b(p3)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        if self.add_skip:\n",
    "            # d1 = self.d1(b, s4)\n",
    "            # d2 = self.d2(d1, s3)\n",
    "            d2 = self.d2(b, s3)\n",
    "            d3 = self.d3(d2, s2)\n",
    "            d4 = self.d4(d3, s1)\n",
    "            d5 = self.d5(d4, s0)\n",
    "        else:\n",
    "            # d1 = self.d1(b)\n",
    "            # d2 = self.d2(d1)\n",
    "            d2 = self.d2(b)\n",
    "            d3 = self.d3(d2)\n",
    "            d4 = self.d4(d3)\n",
    "            d5 = self.d5(d4)\n",
    "\n",
    "        succ_image = self.out(d5)\n",
    "\n",
    "        features_out = self.mlp(torch.cat([b.view(-1, self.bottleneck_size), features], dim=1))\n",
    "        features_out = features_out.view(-1, 2, features_out.shape[1] // 2)\n",
    "        \n",
    "\n",
    "        return succ_image, features_out\n",
    "    \n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A plain conv net for classification / predicting q-values.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, img_size=(512, 512), num_features=6):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        \"\"\" 1st Encoder \"\"\"\n",
    "        self.c0 = ConvBlock(in_channels, 16)\n",
    "        self.c1 = ConvBlock(16, 32)\n",
    "        self.c2 = ConvBlock(32, 64)\n",
    "        self.c3 = ConvBlock(64, 128)\n",
    "        self.c4 = ConvBlock(128, 256)\n",
    "\n",
    "\n",
    "        # compute bottleneck size\n",
    "        x = torch.zeros(1, in_channels, *img_size)\n",
    "        with torch.no_grad():\n",
    "            x = self.c0(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.c1(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.c2(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.c3(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.c4(x)\n",
    "            self.bottleneck_size = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "\n",
    "        self.mlp = MLP(self.bottleneck_size + num_features, 2 * num_features + 1)\n",
    "        \n",
    "    def forward(self, state, features, action, task, obstacles):\n",
    "        # print(state.shape, obstacles.shape, action.shape, task.shape)\n",
    "        input = torch.cat([state, action, task, obstacles], dim=1)\n",
    "        \n",
    "        s0 = self.c0(input)\n",
    "        s0 = self.pool(s0)\n",
    "        s1 = self.c1(s0)\n",
    "        s1 = self.pool(s1)\n",
    "        s2 = self.c2(s1)\n",
    "        s2 = self.pool(s2)\n",
    "        s3 = self.c3(s2)\n",
    "        s3 = self.pool(s3)\n",
    "        s4 = self.c4(s3)\n",
    "\n",
    "        out = self.mlp(torch.cat([s4.view(-1, self.bottleneck_size), features], dim=1))\n",
    "        q = out[:,0]\n",
    "        features_out = out[:,1:]\n",
    "        features_out = features_out.view(-1, 2, features.shape[1])\n",
    "\n",
    "        return q, features_out\n",
    "    \n",
    "\n",
    "def compute_reward(state, task):\n",
    "    \"\"\"\n",
    "    compute the reward based on the state and the task, i.e. the inner product of the state and the task images\n",
    "    \"\"\"\n",
    "    image_features = state.softmax(dim=1)[:,1]\n",
    "    return torch.sum(image_features * task.squeeze(1), dim=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a9796ef137b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:34:50.284387Z",
     "start_time": "2024-02-19T13:34:46.958553Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# testing the Unet\n",
    "model = UNet(4, add_skip=False, img_size=(width, height)).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "model.eval()\n",
    "\n",
    "num_actions = 3\n",
    "# random input of size 512 x 512\n",
    "x = torch.rand(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "o = torch.rand(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "a = torch.rand(num_actions, 1, width, height, device=device)\n",
    "z = torch.ones(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "y = torch.ones(1, 6, device=device).expand((num_actions, -1))\n",
    "\n",
    "img_out, features_out = model(state=x, features=y, action=a, task=z, obstacles=o)\n",
    "print(img_out)\n",
    "print(features_out)\n",
    "\n",
    "img = img_out[0].softmax(dim=0)[1]\n",
    "features = features_out[0].softmax(dim=0)[1]\n",
    "\n",
    "# \n",
    "print(features)\n",
    "plt.imshow(img.detach().cpu().numpy(), cmap='gray')\n",
    "# Q_valu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the SuccessorNet\n",
    "model = SuccessorNet(img_size=(width, height)).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "model.eval()\n",
    "\n",
    "num_actions = 3\n",
    "# random input of size 512 x 512\n",
    "x = torch.rand(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "o = torch.rand(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "a = torch.rand(num_actions, 1, width, height, device=device)\n",
    "z = torch.ones(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "y = torch.ones(1, 6, device=device).expand((num_actions, -1))\n",
    "\n",
    "img_out, features_out = model(state=x, features=y, action=a, task=z, obstacles=o)\n",
    "print(img_out)\n",
    "print(features_out)\n",
    "\n",
    "img = img_out[0].softmax(dim=0)[1]\n",
    "features = features_out[0].softmax(dim=0)[1]\n",
    "\n",
    "# \n",
    "print(features)\n",
    "plt.imshow(img.detach().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6168d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the ConvNet\n",
    "\n",
    "model = ConvNet(4, img_size=(width, height)).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "model.eval()\n",
    "\n",
    "num_actions = 3\n",
    "# random input of size 512 x 512\n",
    "x = torch.rand(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "o = torch.rand(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "a = torch.rand(num_actions, 1, width, height, device=device)\n",
    "z = torch.ones(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "y = torch.ones(1, 6, device=device).expand((num_actions, -1))\n",
    "\n",
    "q, features = model(state=x, features=y, action=a, task=z, obstacles=o)\n",
    "print(q)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2e5234fd731d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:34:50.285234Z",
     "start_time": "2024-02-19T13:34:50.284048Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204f00ce525bfd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:35:43.724574Z",
     "start_time": "2024-02-19T13:35:43.701405Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state_features, binary_features, action_features, task_features, obstacle_features, epsilon=None, return_succ_features=False):\n",
    "    \"\"\"\n",
    "    Implements eps-greedy action selection based on the model's predictions.\n",
    "    If return_succ_features is True, the function returns the index of the selected action and the predicted successor features (for training)\n",
    "    Note that the return values are semantically different for Unet and ConvNet/SuccessorNet models.\n",
    "    \"\"\"\n",
    "    num_actions = action_features.shape[0]\n",
    "\n",
    "    if epsilon is None or random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, (UNet, SuccessorNet)):\n",
    "                succ_img, succ_bin = model(state_features.expand((num_actions, -1, -1, -1)), \n",
    "                                    binary_features.expand((num_actions, -1)), \n",
    "                                    action_features, \n",
    "                                    task_features.expand((num_actions, -1, -1, -1)), \n",
    "                                    obstacle_features.expand((num_actions, -1, -1, -1)))\n",
    "                q_values = compute_reward(succ_img, task_features)\n",
    "                i = torch.argmax(q_values)\n",
    "                if return_succ_features:\n",
    "                    return i, succ_img[i], succ_bin[i]\n",
    "                return i\n",
    "            \n",
    "            elif isinstance(model, ConvNet):\n",
    "                q_values, succ_bin = model(state_features.expand((num_actions, -1, -1, -1)), \n",
    "                                    binary_features.expand((num_actions, -1)), \n",
    "                                    action_features, \n",
    "                                    task_features.expand((num_actions, -1, -1, -1)), \n",
    "                                    obstacle_features.expand((num_actions, -1, -1, -1)))\n",
    "                i = torch.argmax(q_values)\n",
    "                if return_succ_features:\n",
    "                    return i, q_values[i], succ_bin[i]\n",
    "                return i\n",
    "            \n",
    "    else:\n",
    "        i = np.random.randint(0, num_actions)\n",
    "        if return_succ_features:\n",
    "            succ_img, succ_bin = model(state_features.unsqueeze(0),\n",
    "                                  binary_features.unsqueeze(0), \n",
    "                                  action_features[i].unsqueeze(0), \n",
    "                                  task_features.unsqueeze(0), \n",
    "                                  obstacle_features.unsqueeze(0))\n",
    "            return i, succ_img[i], succ_bin[i]\n",
    "        return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e69fee139fde8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:35:43.955040Z",
     "start_time": "2024-02-19T13:35:43.952237Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize_successor_net(policy_net, target_net, optimizer, scheduler, memory, gamma, n_steps=10, batch_size=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform n_steps of optimization on the policy_net (UNet or SuccessorNet) using the transitions in memory.\n",
    "    There are different ways of defining the loss functions for the successor features (commented out below)\n",
    "    \"\"\"\n",
    "\n",
    "    policy_net.train()\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"optimizing for {n_steps} steps...\")\n",
    "    it = tqdm(range(n_steps), disable=not verbose)\n",
    "    losses = []\n",
    "    for i in it:\n",
    "        transitions = memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # policy net prediction for current state\n",
    "        successor_state_features, successor_binary_features = policy_net(torch.stack(batch.state_features),\n",
    "                                                                         torch.stack(batch.binary_features),\n",
    "                                                                         torch.stack(batch.action_features), \n",
    "                                                                         torch.stack(batch.task_features), \n",
    "                                                                         torch.stack(batch.obstacle_features))\n",
    "        \n",
    "        state_action_values = compute_reward(successor_state_features, torch.stack(batch.task_features))\n",
    "        \n",
    "\n",
    "        # Compute next state predictions based on the target net\n",
    "        next_state_values = torch.zeros(batch_size, device=device)\n",
    "        next_state_successor_features = torch.zeros((batch_size, *successor_state_features.shape[-2:]), device=device)\n",
    "        next_state_binary_features = torch.zeros((batch_size, successor_binary_features.shape[-1]), device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # TODO: can we batch this?\n",
    "            for j, transition in enumerate(transitions):\n",
    "                if not transition.done:\n",
    "                    num_actions = transition.next_actions_features.shape[0]\n",
    "                    i, _next_state_succ_img, _next_state_succ_bin = select_action(target_net, state_features=transition.next_state_features.expand((num_actions, -1, -1, -1)), \n",
    "                                              binary_features=transition.next_binary_features.expand((num_actions, -1)), \n",
    "                                              action_features=transition.next_actions_features, \n",
    "                                              task_features=transition.task_features.expand((num_actions, -1, -1, -1)), \n",
    "                                              obstacle_features=transition.obstacle_features.expand((num_actions, -1, -1, -1)), return_succ_features=True)\n",
    "                    \n",
    "                    next_state_values[j] = compute_reward(_next_state_succ_img.unsqueeze(0), transition.task_features.unsqueeze(0))\n",
    "                    next_state_binary_features[j] = _next_state_succ_bin.softmax(dim=0)[1]\n",
    "                    next_state_successor_features[j] = _next_state_succ_img.softmax(dim=0)[1]\n",
    "\n",
    "                else:\n",
    "                    # done state\n",
    "                    # backpropagate zero future reward and static future state\n",
    "                    next_state_values[j] = 0\n",
    "                    next_state_binary_features[j] = transition.next_binary_features\n",
    "                    next_state_successor_features[j] = transition.next_state_features.squeeze()\n",
    "                    \n",
    "        \n",
    "        # define loss functions\n",
    "        mse = nn.MSELoss()\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "              \n",
    "        # reward loss\n",
    "        # loss = mse(state_action_values, torch.stack(batch.reward) + gamma * next_state_values)\n",
    "\n",
    "        # state feature cross-entropy loss\n",
    "        # state_target = (1-gamma) * torch.stack(batch.state_features).squeeze(1) + gamma * next_state_successor_features\n",
    "        # state_target = torch.stack([1 - state_target, state_target], dim=1)\n",
    "        # loss = cross_entropy(successor_state_features, state_target) \n",
    "\n",
    "        # state feature mse loss\n",
    "        state_target = (1-gamma) * torch.stack(batch.state_features).squeeze(1) + gamma * next_state_successor_features\n",
    "        loss = mse(successor_state_features.softmax(dim=1)[:,1], state_target)\n",
    "\n",
    "        # additional features loss\n",
    "        # add_features_target = (1-gamma) * torch.stack(batch.binary_features) + gamma * next_state_binary_features\n",
    "        # loss += cross_entropy(successor_binary_features, torch.stack([1 - add_features_target, add_features_target], dim=1)) \n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        # torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        it.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # free all memory\n",
    "    del transitions, batch, state_action_values, next_state_values, next_state_successor_features, next_state_binary_features\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    policy_net.eval()\n",
    "    return losses\n",
    "    \n",
    "\n",
    "def optimize_convnet(policy_net, target_net, optimizer, scheduler, memory, gamma, n_steps=10, batch_size=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform n_steps of optimization on the policy_net using the transitions in memory.\n",
    "    This is essentially standard Q-learning with a target network.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"optimizing for {n_steps} steps...\")\n",
    "    it = tqdm(range(n_steps), disable=not verbose)\n",
    "    losses = []\n",
    "    for i in it:\n",
    "        transitions = memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # current state q-values predictions\n",
    "        state_action_values, successor_binary_features = policy_net(torch.stack(batch.state_features),\n",
    "                                                                         torch.stack(batch.binary_features),\n",
    "                                                                         torch.stack(batch.action_features), \n",
    "                                                                         torch.stack(batch.task_features), \n",
    "                                                                         torch.stack(batch.obstacle_features))\n",
    "        \n",
    "        # next state q-values and predictions\n",
    "        next_state_values = torch.zeros(batch_size, device=device)\n",
    "        next_state_binary_features = torch.zeros((batch_size, successor_binary_features.shape[-1]), device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # TODO: can we batch this?\n",
    "            for j, transition in enumerate(transitions):\n",
    "                if not transition.done:\n",
    "                    num_actions = transition.next_actions_features.shape[0]\n",
    "                    imax, _next_state_q_values, _next_state_succ_bin = select_action(target_net, state_features=transition.next_state_features.expand((num_actions, -1, -1, -1)), \n",
    "                                              binary_features=transition.next_binary_features.expand((num_actions, -1)), \n",
    "                                              action_features=transition.next_actions_features, \n",
    "                                              task_features=transition.task_features.expand((num_actions, -1, -1, -1)), \n",
    "                                              obstacle_features=transition.obstacle_features.expand((num_actions, -1, -1, -1)), return_succ_features=True)\n",
    "                    \n",
    "                    next_state_values[j] = _next_state_q_values\n",
    "                    next_state_binary_features[j] = _next_state_succ_bin.softmax(dim=0)[1]\n",
    "\n",
    "                else:\n",
    "                    # done state\n",
    "                    # backpropagate zero future reward and static future state\n",
    "                    next_state_values[j] = 0\n",
    "                    next_state_binary_features[j] = transition.next_binary_features\n",
    "                    \n",
    "        \n",
    "        # define loss functions\n",
    "        mse = nn.MSELoss()\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "              \n",
    "        # reward loss\n",
    "        q_target = torch.stack(batch.reward) + gamma * next_state_values\n",
    "        loss = mse(state_action_values, q_target)\n",
    "\n",
    "        # additional features loss\n",
    "        # add_features_target = (1-gamma) * torch.stack(batch.binary_features) + gamma * next_state_binary_features\n",
    "        # loss += cross_entropy(successor_binary_features, torch.stack([1 - add_features_target, add_features_target], dim=1)) \n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        # torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        it.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "    # free all memory\n",
    "    del transitions, batch, state_action_values, next_state_values\n",
    "    if isinstance(policy_net, UNet):\n",
    "        del next_state_successor_features, next_state_binary_features\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aef32cd3731258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:35:44.549570Z",
     "start_time": "2024-02-19T13:35:44.182216Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# free memory\n",
    "gc.collect(), torch.cuda.empty_cache()\n",
    "\n",
    "# initialization for training\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.95\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.01\n",
    "OPT_STEPS = 25  # opt steps per round\n",
    "SHOW_PLOTS = False  # if true, there are more debug outputs in the notebook, so don't run it too long it might crash eventually\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    # weights initialization\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "# UNet: Doesn't really work\n",
    "# add_skip = False\n",
    "# policy_net = UNet(4, add_skip=add_skip, img_size=(width, height)).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "# target_net = UNet(4, add_skip=add_skip, img_size=(width, height)).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "\n",
    "\n",
    "# CNN: Works ok, just goal-conditioned q-learning without successor images\n",
    "        \n",
    "# policy_net = ConvNet(4, img_size=(width, height)).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "# target_net = ConvNet(4, img_size=(width, height)).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "# a smaller learning rate works better for the CNN:\n",
    "# optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "\n",
    "# SuccessorNet: Works well, currently trained to predict successor state images with MSE loss\n",
    "# Cross entropy didn't work well, but maybe it's worth trying again\n",
    "hidden_dims = [256, 128, 64, 128, 256]\n",
    "policy_net = SuccessorNet(img_size=(width, height), hidden_dims=hidden_dims).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "target_net = SuccessorNet(img_size=(width, height), hidden_dims=hidden_dims).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# initialization\n",
    "policy_net.apply(init_weights)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "# scheduler = MultiStepLR(optimizer, milestones=[5000,10000], gamma=0.8)\n",
    "scheduler = None\n",
    "memory = ReplayBuffer(2000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ground placement discretization\n",
    "x_discr_ground = np.linspace(0.2, 0.8, 3)\n",
    "\n",
    "# setup environment\n",
    "env = AssemblyGym(**tower_setup(targets=[(0.5, 0, 0. + 0.05)]), \n",
    "                  reward_fct=sparse_reward,\n",
    "                  restrict_2d=True, \n",
    "                  assembly_env=AssemblyEnv(render=False))\n",
    "state, info = env.reset()\n",
    "\n",
    "# setup logging\n",
    "run = Run(experiment=\"AssemblyDQN\", repo='../aim-data/')\n",
    "run['setup'] = 'tower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6aaa54f569f245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:37:13.862334Z",
     "start_time": "2024-02-19T13:35:44.559102Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# actual training\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 2000\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "def extract_img(logits_img):\n",
    "    return logits_img[0].softmax(dim=0)[1].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "tq = tqdm(range(num_episodes))\n",
    "\n",
    "target_net.eval()\n",
    "policy_net.eval()\n",
    "\n",
    "losses = []\n",
    "for i_episode in tq:\n",
    "    # print(f\"starting episode {i_episode}\")\n",
    "    # Initialize a random task environment and get its state\n",
    "    tower_height = 0.02 + 0.05 * 2 # np.random.randint(0, 3)\n",
    "    observation, info = env.reset(**tower_setup(targets=[(np.random.choice(x_discr_ground, 1, replace=False).item(), 0, tower_height)]))\n",
    "    \n",
    "    # initialize features\n",
    "    task_features = get_task_features(env, width=width, height=height)\n",
    "    task_features = torch.tensor(task_features, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    obstacle_features = get_obstacle_features(env, width=width, height=height)\n",
    "    obstacle_features = torch.tensor(obstacle_features, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    state_features = get_state_features(observation, width=width, height=height)\n",
    "    state_features = torch.tensor(state_features, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    binary_features = get_binary_features(observation)\n",
    "    binary_features = torch.tensor(binary_features, dtype=torch.float32, device=device)\n",
    "\n",
    "    available_actions = [*generate_actions(env, x_discr_ground=x_discr_ground)]\n",
    "    action_features = np.array(action_image_features(env, available_actions, width=width, height=height))\n",
    "    action_features = torch.tensor(action_features, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    available_actions, action_features = reduce_available_actions(state_features, obstacle_features, available_actions, action_features)\n",
    "\n",
    "    # episode\n",
    "    episode_reward = 0\n",
    "    epsilon = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * i_episode / EPS_DECAY)\n",
    "\n",
    "    eval_round = (i_episode % 5) == 0\n",
    "    if eval_round:\n",
    "        epsilon = 0.\n",
    "    \n",
    "    # logging\n",
    "    context = {'eval_round' : eval_round}\n",
    "    run.track(epsilon, name='epsilon', step=i_episode, context=context)\n",
    "    run.track(tower_height, name='tower_height', step=i_episode, context=context)\n",
    "\n",
    "    # maximum of 10 steps in each episode \n",
    "    for t in range(10):\n",
    "    \n",
    "        imax = select_action(model=policy_net, \n",
    "                               state_features=state_features, \n",
    "                               binary_features=binary_features, \n",
    "                               action_features=action_features, \n",
    "                               task_features=task_features, \n",
    "                               obstacle_features=obstacle_features, epsilon=epsilon, return_succ_features=False)\n",
    "        \n",
    "        selected_action, selected_action_features = available_actions[imax], action_features[imax]\n",
    "                               \n",
    "        # if eval_round or i_episode > 0:\n",
    "        if i_episode > 10 and isinstance(policy_net, (SuccessorNet, UNet)):\n",
    "            with torch.no_grad():\n",
    "                fig, axes = plt.subplots(ncols=4, figsize=(5, 10))\n",
    "                (ax1, ax2, ax3, ax4) = axes\n",
    "                ax1.imshow(state_features.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "                ax1.set_title('state')\n",
    "\n",
    "                ax2.imshow(selected_action_features.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "                ax2.set_title('action')\n",
    "\n",
    "                succ_img, succ_bin = policy_net(state_features.unsqueeze(0), \n",
    "                                    binary_features.unsqueeze(0), \n",
    "                                    selected_action_features.unsqueeze(0), \n",
    "                                    task_features.unsqueeze(0), \n",
    "                                    obstacle_features.unsqueeze(0))\n",
    "                ax3.imshow(extract_img(succ_img), cmap='gray', vmin=0, vmax=1)\n",
    "                ax3.set_title('prediction')\n",
    "\n",
    "                # plot task features\n",
    "                ax4.imshow(task_features.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "                ax4.set_title('task')\n",
    "\n",
    "                for ax in axes:\n",
    "                    ax.axis('off')\n",
    "                    ax.invert_yaxis()\n",
    "\n",
    "                if SHOW_PLOTS:\n",
    "                    plt.show()\n",
    "                plt.close(fig)\n",
    "                aim_figure = Image(fig)\n",
    "\n",
    "                run.track(aim_figure, name=f\"pred_step_{t}\", step=i_episode)        \n",
    "                del succ_img, succ_bin\n",
    "        \n",
    "        # environment step\n",
    "        observation, reward, terminated, truncated, info = env.step(selected_action)\n",
    "\n",
    "        # overwrite reward\n",
    "        reward = torch.sum(selected_action_features * task_features) / torch.sum(task_features)\n",
    "        episode_reward += GAMMA**t * reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # compute features\n",
    "        next_state_features = get_state_features(observation, width=width, height=height)\n",
    "        next_binary_features = get_binary_features(observation)\n",
    "        \n",
    "        next_state_features = torch.tensor(next_state_features, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        next_binary_features = torch.tensor(next_binary_features, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # compute available_actions and features\n",
    "        next_available_actions = [*generate_actions(env, x_discr_ground=x_discr_ground)]\n",
    "        next_actions_features = np.array(action_image_features(env, next_available_actions, width=width, height=height))\n",
    "        next_actions_features = torch.tensor(next_actions_features, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_available_actions, next_actions_features = reduce_available_actions(next_state_features, obstacle_features, next_available_actions, next_actions_features)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        transition = Transition(state_features=state_features, \n",
    "                                binary_features=binary_features,\n",
    "                                action_features=selected_action_features, \n",
    "                                action=selected_action,\n",
    "                                reward=reward, \n",
    "                                next_state_features=next_state_features, \n",
    "                                next_binary_features=next_binary_features,\n",
    "                                next_available_actions=next_available_actions,\n",
    "                                next_actions_features=next_actions_features,\n",
    "                                task_features=task_features,\n",
    "                                obstacle_features=obstacle_features,\n",
    "                                done=done)\n",
    "        memory.push(transition)\n",
    "\n",
    "        # Move to the next state        \n",
    "        state_features = next_state_features\n",
    "        binary_features = next_binary_features\n",
    "        available_actions = next_available_actions\n",
    "        action_features = next_actions_features\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if i_episode > 10:\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        policy_net.train()\n",
    "        if isinstance(policy_net, (UNet, SuccessorNet)):\n",
    "            losses += optimize_successor_net(policy_net=policy_net,\n",
    "                        target_net=target_net,\n",
    "                        optimizer=optimizer, \n",
    "                        scheduler=scheduler,\n",
    "                        memory=memory,\n",
    "                        gamma=GAMMA,\n",
    "                        n_steps=OPT_STEPS, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        verbose=SHOW_PLOTS)\n",
    "        elif isinstance(policy_net, ConvNet):\n",
    "            losses += optimize_convnet(policy_net=policy_net,\n",
    "                        target_net=target_net,\n",
    "                        optimizer=optimizer, \n",
    "                        scheduler=scheduler,\n",
    "                        memory=memory,\n",
    "                        gamma=GAMMA,\n",
    "                        n_steps=OPT_STEPS, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        verbose=True)\n",
    "\n",
    "        policy_net.eval()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        #     + (1  )\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\n",
    "    # free memory\n",
    "    del state_features, binary_features, action_features, next_state_features, next_binary_features, next_actions_features\n",
    "    del task_features, obstacle_features\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # print(f\"Memory size: {memory.size_in_MB()} MB; Total used memory: {torch.cuda.memory_allocated(0) / 1024 / 1024} MB\")\n",
    "\n",
    "    # logging\n",
    "    run.track(episode_reward, name='reward', step=i_episode, context=context)\n",
    "    run.track(t, name='num_steps', step=i_episode, context=context)\n",
    "    \n",
    "    # track memory usage\n",
    "    run.track(memory.size_in_MB(), name='replay_buffer_size', step=i_episode, context=context)\n",
    "    run.track(torch.cuda.memory_allocated(0) / 1024 / 1024, name='total_gpu_memory', step=i_episode, context=context)\n",
    "\n",
    "    # print(f\"episode reward: {episode_reward}, steps taken: {t}, tower height: {tower_height}, epsilon: {epsilon}\")\n",
    "    tq.set_postfix(reward=episode_reward.cpu().numpy(), \n",
    "                    steps=t, \n",
    "                    height=tower_height, \n",
    "                    epsilon=epsilon,\n",
    "                     memory=f\"{memory.size_in_MB():.0f} MB\", \n",
    "                     total_memory=f\"{torch.cuda.memory_allocated(0) / 1024 / 1024:.0f} MB\")\n",
    "    episode_rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8150d5",
   "metadata": {},
   "source": [
    "# Code to debug the training, e.g. by fitting on a set of transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5274f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transitions = memory.sample(3)\n",
    "\n",
    "# for i, transition in enumerate(transitions):\n",
    "#     print(transition.reward, transition.done)\n",
    "# plot state features of each transition\n",
    "# fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(10, 15))\n",
    "# for i, transition in enumerate(transitions):\n",
    "#     (ax1, ax2, ax3, ax4) = axes[i]\n",
    "#     ax1.imshow(transition.state_features.squeeze().cpu().numpy(), cmap='gray')\n",
    "#     ax1.set_title('state')\n",
    "\n",
    "#     ax2.imshow(transition.action_features.squeeze().cpu().numpy(), cmap='gray')\n",
    "#     ax2.set_title('action')\n",
    "\n",
    "#     ax3.imshow(transition.next_state_features.squeeze().cpu().numpy(), cmap='gray')\n",
    "#     ax3.set_title('next state')\n",
    "\n",
    "#     # ax4.imshow(transition.next_binary_features.cpu().numpy(), cmap='gray')\n",
    "#     # ax4.set_title('next binary')\n",
    "\n",
    "#     for ax in axes[i]:\n",
    "#         ax.axis('off')\n",
    "#         ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(memory))\n",
    "losses = []\n",
    "# policy_net = ConvNet(4, img_size=(width, height)).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "# policy_net.apply(init_weights)\n",
    "\n",
    "# target_net = ConvNet(4, img_size=(width, height)).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "add_skip=True\n",
    "# policy_net = UNet(4, add_skip=add_skip, img_size=(width, height)).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "# policy_net.apply(init_weights)\n",
    "\n",
    "# target_net = UNet(4, add_skip=add_skip, img_size=(width, height)).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "hidden_dims = [256, 128, 64, 128, 256]\n",
    "policy_net = SuccessorNet(img_size=(width, height), hidden_dims=hidden_dims).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "policy_net.apply(init_weights)\n",
    "\n",
    "target_net = SuccessorNet(img_size=(width, height), hidden_dims=hidden_dims).to(device) # DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa84c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.0001) \n",
    "\n",
    "policy_net.train()\n",
    "losses += optimize_successor_net(policy_net=policy_net,\n",
    "                        target_net=target_net,\n",
    "                        optimizer=optimizer, \n",
    "                        scheduler=scheduler,\n",
    "                        memory=memory,\n",
    "                        gamma=GAMMA,\n",
    "                        n_steps=500, \n",
    "                        batch_size=32, \n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "\n",
    "# sample predictions\n",
    "\n",
    "num_predictions = 10\n",
    "transitions = memory.sample(num_predictions)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_predictions, ncols=4, figsize=(4*2, num_predictions*2 ))\n",
    "for i, transition in enumerate(transitions):\n",
    "    (ax1, ax2, ax3, ax4) = axes[i]\n",
    "    ax1.imshow(transition.state_features.squeeze().cpu().numpy(), cmap='gray')\n",
    "    ax1.set_title('state')\n",
    "\n",
    "    ax2.imshow(transition.action_features.squeeze().cpu().numpy(), cmap='gray')\n",
    "    ax2.set_title('action')\n",
    "\n",
    "    succ_img, succ_bin = policy_net(transition.state_features.unsqueeze(0), \n",
    "                                    transition.binary_features.unsqueeze(0), \n",
    "                                    transition.action_features.unsqueeze(0), \n",
    "                                    transition.task_features.unsqueeze(0), \n",
    "                                    transition.obstacle_features.unsqueeze(0))\n",
    "    ax3.imshow(extract_img(succ_img), cmap='gray')\n",
    "    ax3.set_title('prediction')\n",
    "\n",
    "    ax4.imshow(transition.task_features.squeeze().cpu().numpy(), cmap='gray')\n",
    "    ax4.set_title('task')\n",
    "\n",
    "    for ax in axes[i]:\n",
    "        ax.axis('off')\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c49586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Generate a random target tensor\n",
    "# num_actions = 1\n",
    "\n",
    "# x = torch.rand(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "# o = torch.rand(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "# a = torch.rand(num_actions, 1, width, height, device=device)\n",
    "# z = torch.ones(1, 1, width, height, device=device).expand((num_actions, -1, -1 ,-1))\n",
    "# y = torch.ones(1, 6, device=device).expand((num_actions, -1))\n",
    "\n",
    "\n",
    "# target = torch.rand(num_actions, 1, height, width, device=device)\n",
    "# print(target.shape)\n",
    "# target = torch.tensor(task, device=device, dtype=torch.float32).view(num_actions, 1, height, width)\n",
    "# print(target)\n",
    "\n",
    "\n",
    "# # plot target\n",
    "# plt.imshow(target[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "\n",
    "\n",
    "# # Define the network architecture\n",
    "# model = UNet(4, add_skip=False, img_size=(width, height)).to(device)\n",
    "# model.apply(init_weights)\n",
    "\n",
    "# # Define the loss function\n",
    "# # criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Define the optimizer\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# # use Adam optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 200\n",
    "# it = tqdm(range(num_epochs))\n",
    "# for epoch in it:\n",
    "#     # Forward pass\n",
    "#     output, _ = model(state=x, features=y, action=a, task=z, obstacles=o)\n",
    "#     # print(output.shape, target.shape)\n",
    "\n",
    "#     # Compute the loss\n",
    "#     loss = criterion(output, target.squeeze(1).long())\n",
    "    \n",
    "#     # Backward pass\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     # Print the loss for monitoring\n",
    "#     it.set_postfix(loss=loss.item())\n",
    "\n",
    "#     if epoch % 100 == 0:\n",
    "#         # plot\n",
    "#         img = output[0].softmax(dim=0)[1]\n",
    "\n",
    "#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#         ax1.imshow(target[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "#         ax1.set_title('Target')\n",
    "#         ax2.imshow(img.detach().cpu().numpy(), cmap='gray')\n",
    "#         ax2.set_title('Output')\n",
    "#         plt.show(fig)\n",
    "\n",
    "#     # (f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# # Evaluate the trained network on the target tensor\n",
    "# # output, _ = model(x, y, a, z, o)\n",
    "# # evaluation_loss = criterion(output, target.squeeze(1).long())\n",
    "# # print(f\"Evaluation Loss: {evaluation_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981eb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(in_dim=10, out_dim=10, hidden_dim=[2, 2]):\n",
    "    hidden_dim.append(out_dim)\n",
    "\n",
    "    for cur_dim in hidden_dim:\n",
    "        print(f\"adding layer {in_dim, cur_dim}\")\n",
    "        in_dim = cur_dim\n",
    "\n",
    "print(\"first MLP\")\n",
    "MLP()\n",
    "print(\"second MLP\")\n",
    "MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03418a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
